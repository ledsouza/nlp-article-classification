{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import spacy.lang.pt\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def download_csv(url, data_dir='./../data') -> pd.DataFrame:\n",
    "    # Create cache directory if not exists\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate a safe filename from URL\n",
    "    filename = Path(data_dir) / url.split('/')[-1]\n",
    "    \n",
    "    # Check if file already exists locally\n",
    "    if filename.exists():\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    try:\n",
    "        # Advanced request with timeout and proper headers\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 Academic Data Retrieval',\n",
    "            'Accept': 'text/csv',\n",
    "        }\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            headers=headers, \n",
    "            timeout=300,  # 300 seconds timeout\n",
    "            stream=True  # Memory efficient for large files\n",
    "        )\n",
    "        \n",
    "        # Raise an exception for bad status codes\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save to local cache\n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error: {e}.\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_valid_tokens(doc: Doc) -> str | None:\n",
    "    valid_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        is_valid = token.is_alpha and not token.is_stop\n",
    "        if is_valid:\n",
    "            valid_tokens.append(token.text)\n",
    "\n",
    "    if len(valid_tokens) > 2:\n",
    "        return \" \".join(valid_tokens)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text: str, nlp: spacy.lang.pt.Portuguese) -> list[str]:\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        is_valid = token.is_alpha and not token.is_stop\n",
    "        if is_valid:\n",
    "            tokens.append(token.text.lower())\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_tokens_vector(tokens: list[str], model: KeyedVectors) -> list[float]:\n",
    "    vector = np.zeros((1, 300))\n",
    "\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vector += model.get_vector(token)\n",
    "        except KeyError:\n",
    "            continue # If the token doesn't exist on the vocab, jump to the next token\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(texts: pd.Series, model: KeyedVectors, nlp: spacy.lang.pt.Portuguese):\n",
    "    vectors = np.zeros((len(texts), 300))\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = tokenizer(text, nlp)\n",
    "        vectors[i] = sum_tokens_vector(tokens, model)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_classifier(x_train: pd.Series | np.ndarray,\n",
    "                  y_train: pd.Series | np.ndarray,\n",
    "                  x_test: pd.Series | np.ndarray,\n",
    "                  y_test: pd.Series | np.ndarray) -> LogisticRegression:\n",
    "\n",
    "    lr_model = LogisticRegression(max_iter=800)\n",
    "    lr_model.fit(x_train, y_train)\n",
    "    labels = lr_model.predict(x_test)\n",
    "    results = classification_report(y_test, labels)\n",
    "    print(results)\n",
    "    return lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = download_csv(\"https://cdn3.gnarususercontent.com.br/1638-word-embedding/treino.csv\")\n",
    "df_test = download_csv(\"https://cdn3.gnarususercontent.com.br/1638-word-embedding/teste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29095</th>\n",
       "      <td>Animação 'Vida de Abobrinha' é afetuosa sem se...</td>\n",
       "      <td>Icare –ou Abobrinha, como prefere ser chamado–...</td>\n",
       "      <td>2017-02-19</td>\n",
       "      <td>ilustrada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/ilustrada/2017/02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22458</th>\n",
       "      <td>Em cinco minutos, croata vence jogo interrompi...</td>\n",
       "      <td>O croata Borna Coric precisou de cinco minutos...</td>\n",
       "      <td>2015-09-19</td>\n",
       "      <td>esporte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/esporte/2015/09/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45655</th>\n",
       "      <td>Polícia filipina usa hospitais para ocultar mo...</td>\n",
       "      <td>Os residentes do bairro Old Balara se esconder...</td>\n",
       "      <td>2017-06-30</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2017/06/189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89662</th>\n",
       "      <td>Em raro pronunciamento, imperador japonês indi...</td>\n",
       "      <td>O imperador do Japão, Akihito, expressou preoc...</td>\n",
       "      <td>2016-08-08</td>\n",
       "      <td>mundo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/mundo/2016/08/180...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13516</th>\n",
       "      <td>Casa Folha trará conversas gratuitas com autor...</td>\n",
       "      <td>Durante a Flip, a Casa Folha manterá uma progr...</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>ilustrada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www1.folha.uol.com.br/ilustrada/2017/07...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "29095  Animação 'Vida de Abobrinha' é afetuosa sem se...   \n",
       "22458  Em cinco minutos, croata vence jogo interrompi...   \n",
       "45655  Polícia filipina usa hospitais para ocultar mo...   \n",
       "89662  Em raro pronunciamento, imperador japonês indi...   \n",
       "13516  Casa Folha trará conversas gratuitas com autor...   \n",
       "\n",
       "                                                    text        date  \\\n",
       "29095  Icare –ou Abobrinha, como prefere ser chamado–...  2017-02-19   \n",
       "22458  O croata Borna Coric precisou de cinco minutos...  2015-09-19   \n",
       "45655  Os residentes do bairro Old Balara se esconder...  2017-06-30   \n",
       "89662  O imperador do Japão, Akihito, expressou preoc...  2016-08-08   \n",
       "13516  Durante a Flip, a Casa Folha manterá uma progr...  2017-07-26   \n",
       "\n",
       "        category subcategory  \\\n",
       "29095  ilustrada         NaN   \n",
       "22458    esporte         NaN   \n",
       "45655      mundo         NaN   \n",
       "89662      mundo         NaN   \n",
       "13516  ilustrada         NaN   \n",
       "\n",
       "                                                    link  \n",
       "29095  http://www1.folha.uol.com.br/ilustrada/2017/02...  \n",
       "22458  http://www1.folha.uol.com.br/esporte/2015/09/1...  \n",
       "45655  http://www1.folha.uol.com.br/mundo/2017/06/189...  \n",
       "89662  http://www1.folha.uol.com.br/mundo/2016/08/180...  \n",
       "13516  http://www1.folha.uol.com.br/ilustrada/2017/07...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df_train[\"title\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "cleaned_texts = []\n",
    "for doc in nlp.pipe(train_texts, batch_size=1000, n_process=-1):\n",
    "    cleaned_texts.append(get_text_from_valid_tokens(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7630</th>\n",
       "      <td>escritor joão gilberto noll morre anos porto a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5295</th>\n",
       "      <td>exportação games brasileiros aumentar ano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83436</th>\n",
       "      <td>robinho ressalta atuação ceni fala ansiedade h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80114</th>\n",
       "      <td>terry crews defende comédia feminismo brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81537</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title\n",
       "7630   escritor joão gilberto noll morre anos porto a...\n",
       "5295           exportação games brasileiros aumentar ano\n",
       "83436  robinho ressalta atuação ceni fala ansiedade h...\n",
       "80114     terry crews defende comédia feminismo brooklyn\n",
       "81537                                               None"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_cleaned = pd.DataFrame({\"Title\": cleaned_texts})\n",
    "df_train_cleaned.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90000 entries, 0 to 89999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Title   84680 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 703.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 84466 entries, 0 to 89999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Title   84466 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train_cleaned = df_train_cleaned.dropna()\n",
    "df_train_cleaned = df_train_cleaned.drop_duplicates()\n",
    "df_train_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df_train_cleaned[\"Title\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sg=0, window=2, vector_size=300, min_count=5, alpha=0.03, min_alpha=0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s : - %(message)s\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:12:18,964 : - collecting all words and their counts\n",
      "2024-12-02 15:12:18,964 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-12-02 15:12:18,969 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
      "2024-12-02 15:12:18,973 : - PROGRESS: at sentence #10000, processed 63848 words, keeping 14989 word types\n",
      "2024-12-02 15:12:18,976 : - PROGRESS: at sentence #15000, processed 95753 words, keeping 18279 word types\n",
      "2024-12-02 15:12:18,982 : - PROGRESS: at sentence #20000, processed 127689 words, keeping 21033 word types\n",
      "2024-12-02 15:12:18,988 : - PROGRESS: at sentence #25000, processed 159589 words, keeping 23491 word types\n",
      "2024-12-02 15:12:18,992 : - PROGRESS: at sentence #30000, processed 191554 words, keeping 25494 word types\n",
      "2024-12-02 15:12:18,997 : - PROGRESS: at sentence #35000, processed 223412 words, keeping 27330 word types\n",
      "2024-12-02 15:12:19,001 : - PROGRESS: at sentence #40000, processed 255282 words, keeping 29053 word types\n",
      "2024-12-02 15:12:19,007 : - PROGRESS: at sentence #45000, processed 287297 words, keeping 30606 word types\n",
      "2024-12-02 15:12:19,013 : - PROGRESS: at sentence #50000, processed 319258 words, keeping 31964 word types\n",
      "2024-12-02 15:12:19,018 : - PROGRESS: at sentence #55000, processed 351437 words, keeping 33270 word types\n",
      "2024-12-02 15:12:19,025 : - PROGRESS: at sentence #60000, processed 383579 words, keeping 34520 word types\n",
      "2024-12-02 15:12:19,030 : - PROGRESS: at sentence #65000, processed 415565 words, keeping 35643 word types\n",
      "2024-12-02 15:12:19,036 : - PROGRESS: at sentence #70000, processed 447646 words, keeping 36719 word types\n",
      "2024-12-02 15:12:19,043 : - PROGRESS: at sentence #75000, processed 479568 words, keeping 37802 word types\n",
      "2024-12-02 15:12:19,052 : - PROGRESS: at sentence #80000, processed 511645 words, keeping 38814 word types\n",
      "2024-12-02 15:12:19,059 : - collected 39693 word types from a corpus of 540242 raw words and 84466 sentences\n",
      "2024-12-02 15:12:19,059 : - Creating a fresh vocabulary\n",
      "2024-12-02 15:12:19,086 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12924 unique words (32.56% of original 39693, drops 26769)', 'datetime': '2024-12-02T15:12:19.086502', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-12-02 15:12:19,086 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 495223 word corpus (91.67% of original 540242, drops 45019)', 'datetime': '2024-12-02T15:12:19.086929', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-12-02 15:12:19,112 : - deleting the raw counts dictionary of 39693 items\n",
      "2024-12-02 15:12:19,113 : - sample=0.001 downsamples 8 most-common words\n",
      "2024-12-02 15:12:19,113 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 486147.7552334345 word corpus (98.2%% of prior 495223)', 'datetime': '2024-12-02T15:12:19.113753', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-12-02 15:12:19,156 : - estimated required memory for 12924 words and 300 dimensions: 37479600 bytes\n",
      "2024-12-02 15:12:19,156 : - resetting layer weights\n",
      "2024-12-02 15:12:19,170 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-02T15:12:19.170251', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'build_vocab'}\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(tokens, progress_per=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 15:32:09,352 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12924 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=2 shrink_windows=True', 'datetime': '2024-12-02T15:32:09.352773', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-12-02 15:32:09,789 : - EPOCH 0: training on 540242 raw words (485990 effective words) took 0.4s, 1184025 effective words/s\n",
      "2024-12-02 15:32:10,192 : - EPOCH 1: training on 540242 raw words (486141 effective words) took 0.4s, 1234346 effective words/s\n",
      "2024-12-02 15:32:10,576 : - EPOCH 2: training on 540242 raw words (486209 effective words) took 0.4s, 1282820 effective words/s\n",
      "2024-12-02 15:32:10,966 : - EPOCH 3: training on 540242 raw words (486093 effective words) took 0.4s, 1262589 effective words/s\n",
      "2024-12-02 15:32:11,337 : - EPOCH 4: training on 540242 raw words (485969 effective words) took 0.4s, 1355544 effective words/s\n",
      "2024-12-02 15:32:11,706 : - EPOCH 5: training on 540242 raw words (486205 effective words) took 0.4s, 1336750 effective words/s\n",
      "2024-12-02 15:32:12,075 : - EPOCH 6: training on 540242 raw words (486063 effective words) took 0.4s, 1334761 effective words/s\n",
      "2024-12-02 15:32:12,469 : - EPOCH 7: training on 540242 raw words (486217 effective words) took 0.4s, 1250984 effective words/s\n",
      "2024-12-02 15:32:12,868 : - EPOCH 8: training on 540242 raw words (486187 effective words) took 0.4s, 1258528 effective words/s\n",
      "2024-12-02 15:32:13,263 : - EPOCH 9: training on 540242 raw words (486172 effective words) took 0.4s, 1248095 effective words/s\n",
      "2024-12-02 15:32:13,679 : - EPOCH 10: training on 540242 raw words (486112 effective words) took 0.4s, 1183109 effective words/s\n",
      "2024-12-02 15:32:14,112 : - EPOCH 11: training on 540242 raw words (486004 effective words) took 0.4s, 1134458 effective words/s\n",
      "2024-12-02 15:32:14,471 : - EPOCH 12: training on 540242 raw words (486158 effective words) took 0.4s, 1375433 effective words/s\n",
      "2024-12-02 15:32:14,833 : - EPOCH 13: training on 540242 raw words (486171 effective words) took 0.4s, 1362958 effective words/s\n",
      "2024-12-02 15:32:15,188 : - EPOCH 14: training on 540242 raw words (486210 effective words) took 0.4s, 1388391 effective words/s\n",
      "2024-12-02 15:32:15,554 : - EPOCH 15: training on 540242 raw words (486131 effective words) took 0.4s, 1346705 effective words/s\n",
      "2024-12-02 15:32:15,911 : - EPOCH 16: training on 540242 raw words (486147 effective words) took 0.4s, 1381500 effective words/s\n",
      "2024-12-02 15:32:16,264 : - EPOCH 17: training on 540242 raw words (486153 effective words) took 0.3s, 1432599 effective words/s\n",
      "2024-12-02 15:32:16,616 : - EPOCH 18: training on 540242 raw words (486158 effective words) took 0.3s, 1428681 effective words/s\n",
      "2024-12-02 15:32:16,966 : - EPOCH 19: training on 540242 raw words (486074 effective words) took 0.3s, 1406262 effective words/s\n",
      "2024-12-02 15:32:17,330 : - EPOCH 20: training on 540242 raw words (486189 effective words) took 0.4s, 1356752 effective words/s\n",
      "2024-12-02 15:32:17,680 : - EPOCH 21: training on 540242 raw words (486043 effective words) took 0.3s, 1406004 effective words/s\n",
      "2024-12-02 15:32:18,032 : - EPOCH 22: training on 540242 raw words (486175 effective words) took 0.3s, 1402474 effective words/s\n",
      "2024-12-02 15:32:18,382 : - EPOCH 23: training on 540242 raw words (486123 effective words) took 0.3s, 1409022 effective words/s\n",
      "2024-12-02 15:32:18,731 : - EPOCH 24: training on 540242 raw words (486169 effective words) took 0.3s, 1412189 effective words/s\n",
      "2024-12-02 15:32:19,078 : - EPOCH 25: training on 540242 raw words (485991 effective words) took 0.3s, 1440807 effective words/s\n",
      "2024-12-02 15:32:19,426 : - EPOCH 26: training on 540242 raw words (486110 effective words) took 0.3s, 1449010 effective words/s\n",
      "2024-12-02 15:32:19,776 : - EPOCH 27: training on 540242 raw words (486164 effective words) took 0.3s, 1427510 effective words/s\n",
      "2024-12-02 15:32:20,131 : - EPOCH 28: training on 540242 raw words (486168 effective words) took 0.3s, 1389917 effective words/s\n",
      "2024-12-02 15:32:20,479 : - EPOCH 29: training on 540242 raw words (486159 effective words) took 0.3s, 1417501 effective words/s\n",
      "2024-12-02 15:32:20,479 : - Word2Vec lifecycle event {'msg': 'training on 16207260 raw words (14583855 effective words) took 11.1s, 1310932 effective words/s', 'datetime': '2024-12-02T15:32:20.479445', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14583855, 16207260)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(tokens, total_examples=model.corpus_count, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 16:00:20,778 : - Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2024-12-02T16:00:20.778046', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2024-12-02 16:00:20,779 : - collecting all words and their counts\n",
      "2024-12-02 16:00:20,781 : - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-12-02 16:00:20,793 : - PROGRESS: at sentence #5000, processed 31930 words, keeping 10193 word types\n",
      "2024-12-02 16:00:20,839 : - PROGRESS: at sentence #10000, processed 63848 words, keeping 14989 word types\n",
      "2024-12-02 16:00:20,851 : - PROGRESS: at sentence #15000, processed 95753 words, keeping 18279 word types\n",
      "2024-12-02 16:00:20,857 : - PROGRESS: at sentence #20000, processed 127689 words, keeping 21033 word types\n",
      "2024-12-02 16:00:20,862 : - PROGRESS: at sentence #25000, processed 159589 words, keeping 23491 word types\n",
      "2024-12-02 16:00:20,870 : - PROGRESS: at sentence #30000, processed 191554 words, keeping 25494 word types\n",
      "2024-12-02 16:00:20,876 : - PROGRESS: at sentence #35000, processed 223412 words, keeping 27330 word types\n",
      "2024-12-02 16:00:20,882 : - PROGRESS: at sentence #40000, processed 255282 words, keeping 29053 word types\n",
      "2024-12-02 16:00:20,888 : - PROGRESS: at sentence #45000, processed 287297 words, keeping 30606 word types\n",
      "2024-12-02 16:00:20,893 : - PROGRESS: at sentence #50000, processed 319258 words, keeping 31964 word types\n",
      "2024-12-02 16:00:20,898 : - PROGRESS: at sentence #55000, processed 351437 words, keeping 33270 word types\n",
      "2024-12-02 16:00:20,904 : - PROGRESS: at sentence #60000, processed 383579 words, keeping 34520 word types\n",
      "2024-12-02 16:00:20,909 : - PROGRESS: at sentence #65000, processed 415565 words, keeping 35643 word types\n",
      "2024-12-02 16:00:20,914 : - PROGRESS: at sentence #70000, processed 447646 words, keeping 36719 word types\n",
      "2024-12-02 16:00:20,921 : - PROGRESS: at sentence #75000, processed 479568 words, keeping 37802 word types\n",
      "2024-12-02 16:00:20,926 : - PROGRESS: at sentence #80000, processed 511645 words, keeping 38814 word types\n",
      "2024-12-02 16:00:20,929 : - collected 39693 word types from a corpus of 540242 raw words and 84466 sentences\n",
      "2024-12-02 16:00:20,930 : - Creating a fresh vocabulary\n",
      "2024-12-02 16:00:20,952 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 12924 unique words (32.56% of original 39693, drops 26769)', 'datetime': '2024-12-02T16:00:20.952075', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-12-02 16:00:20,952 : - Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 495223 word corpus (91.67% of original 540242, drops 45019)', 'datetime': '2024-12-02T16:00:20.952814', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-12-02 16:00:20,980 : - deleting the raw counts dictionary of 39693 items\n",
      "2024-12-02 16:00:20,981 : - sample=0.001 downsamples 8 most-common words\n",
      "2024-12-02 16:00:20,982 : - Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 486147.7552334345 word corpus (98.2%% of prior 495223)', 'datetime': '2024-12-02T16:00:20.982381', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2024-12-02 16:00:21,026 : - estimated required memory for 12924 words and 300 dimensions: 37479600 bytes\n",
      "2024-12-02 16:00:21,026 : - resetting layer weights\n",
      "2024-12-02 16:00:21,039 : - Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-02T16:00:21.039694', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "2024-12-02 16:00:21,040 : - Word2Vec lifecycle event {'msg': 'training model with 3 workers on 12924 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-02T16:00:21.040307', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2024-12-02 16:00:22,030 : - EPOCH 0: training on 540242 raw words (486058 effective words) took 1.0s, 493726 effective words/s\n",
      "2024-12-02 16:00:23,036 : - EPOCH 1 - PROGRESS: at 98.14% examples, 476805 words/s, in_qsize 1, out_qsize 1\n",
      "2024-12-02 16:00:23,038 : - EPOCH 1: training on 540242 raw words (486092 effective words) took 1.0s, 485061 effective words/s\n",
      "2024-12-02 16:00:24,071 : - EPOCH 2 - PROGRESS: at 96.23% examples, 455042 words/s, in_qsize 3, out_qsize 0\n",
      "2024-12-02 16:00:24,079 : - EPOCH 2: training on 540242 raw words (486230 effective words) took 1.0s, 469316 effective words/s\n",
      "2024-12-02 16:00:25,116 : - EPOCH 3 - PROGRESS: at 96.23% examples, 453191 words/s, in_qsize 3, out_qsize 0\n",
      "2024-12-02 16:00:25,122 : - EPOCH 3: training on 540242 raw words (486202 effective words) took 1.0s, 468204 effective words/s\n",
      "2024-12-02 16:00:26,166 : - EPOCH 4 - PROGRESS: at 96.23% examples, 450428 words/s, in_qsize 3, out_qsize 0\n",
      "2024-12-02 16:00:26,173 : - EPOCH 4: training on 540242 raw words (486120 effective words) took 1.0s, 464800 effective words/s\n",
      "2024-12-02 16:00:27,230 : - EPOCH 5 - PROGRESS: at 90.67% examples, 418835 words/s, in_qsize 5, out_qsize 0\n",
      "2024-12-02 16:00:27,292 : - EPOCH 5: training on 540242 raw words (486177 effective words) took 1.1s, 436452 effective words/s\n",
      "2024-12-02 16:00:28,303 : - EPOCH 6 - PROGRESS: at 98.14% examples, 476533 words/s, in_qsize 1, out_qsize 1\n",
      "2024-12-02 16:00:28,305 : - EPOCH 6: training on 540242 raw words (486091 effective words) took 1.0s, 484478 effective words/s\n",
      "2024-12-02 16:00:29,268 : - EPOCH 7: training on 540242 raw words (486150 effective words) took 1.0s, 507130 effective words/s\n",
      "2024-12-02 16:00:30,207 : - EPOCH 8: training on 540242 raw words (486136 effective words) took 0.9s, 520581 effective words/s\n",
      "2024-12-02 16:00:31,146 : - EPOCH 9: training on 540242 raw words (486222 effective words) took 0.9s, 521092 effective words/s\n",
      "2024-12-02 16:00:32,080 : - EPOCH 10: training on 540242 raw words (486050 effective words) took 0.9s, 523312 effective words/s\n",
      "2024-12-02 16:00:33,013 : - EPOCH 11: training on 540242 raw words (486153 effective words) took 0.9s, 523880 effective words/s\n",
      "2024-12-02 16:00:33,937 : - EPOCH 12: training on 540242 raw words (486100 effective words) took 0.9s, 528857 effective words/s\n",
      "2024-12-02 16:00:34,862 : - EPOCH 13: training on 540242 raw words (486101 effective words) took 0.9s, 528328 effective words/s\n",
      "2024-12-02 16:00:35,786 : - EPOCH 14: training on 540242 raw words (486192 effective words) took 0.9s, 529039 effective words/s\n",
      "2024-12-02 16:00:36,720 : - EPOCH 15: training on 540242 raw words (486212 effective words) took 0.9s, 522855 effective words/s\n",
      "2024-12-02 16:00:37,638 : - EPOCH 16: training on 540242 raw words (486138 effective words) took 0.9s, 533526 effective words/s\n",
      "2024-12-02 16:00:38,551 : - EPOCH 17: training on 540242 raw words (486139 effective words) took 0.9s, 535045 effective words/s\n",
      "2024-12-02 16:00:39,467 : - EPOCH 18: training on 540242 raw words (486152 effective words) took 0.9s, 533540 effective words/s\n",
      "2024-12-02 16:00:40,380 : - EPOCH 19: training on 540242 raw words (486287 effective words) took 0.9s, 535537 effective words/s\n",
      "2024-12-02 16:00:41,284 : - EPOCH 20: training on 540242 raw words (486202 effective words) took 0.9s, 540592 effective words/s\n",
      "2024-12-02 16:00:42,193 : - EPOCH 21: training on 540242 raw words (486033 effective words) took 0.9s, 537550 effective words/s\n",
      "2024-12-02 16:00:43,096 : - EPOCH 22: training on 540242 raw words (486149 effective words) took 0.9s, 541714 effective words/s\n",
      "2024-12-02 16:00:43,996 : - EPOCH 23: training on 540242 raw words (486188 effective words) took 0.9s, 542740 effective words/s\n",
      "2024-12-02 16:00:44,900 : - EPOCH 24: training on 540242 raw words (486090 effective words) took 0.9s, 541454 effective words/s\n",
      "2024-12-02 16:00:45,802 : - EPOCH 25: training on 540242 raw words (486108 effective words) took 0.9s, 542487 effective words/s\n",
      "2024-12-02 16:00:46,702 : - EPOCH 26: training on 540242 raw words (486111 effective words) took 0.9s, 543050 effective words/s\n",
      "2024-12-02 16:00:47,599 : - EPOCH 27: training on 540242 raw words (486206 effective words) took 0.9s, 545422 effective words/s\n",
      "2024-12-02 16:00:48,492 : - EPOCH 28: training on 540242 raw words (485975 effective words) took 0.9s, 548195 effective words/s\n",
      "2024-12-02 16:00:49,389 : - EPOCH 29: training on 540242 raw words (486236 effective words) took 0.9s, 544932 effective words/s\n",
      "2024-12-02 16:00:49,389 : - Word2Vec lifecycle event {'msg': 'training on 16207260 raw words (14584300 effective words) took 28.4s, 514427 effective words/s', 'datetime': '2024-12-02T16:00:49.389927', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14584300, 16207260)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg = Word2Vec(sg=1,\n",
    "                    window=5,\n",
    "                    vector_size=300,\n",
    "                    min_count=5,\n",
    "                    alpha=0.03,\n",
    "                    min_alpha=0.007)\n",
    "\n",
    "model_sg.build_vocab(tokens, progress_per=5000)\n",
    "\n",
    "model_sg.train(tokens,\n",
    "               total_examples=model_sg.corpus_count,\n",
    "               epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 11:19:37,679 : - storing 12924x300 projection weights into ./../models/model_cbow.txt\n",
      "2024-12-03 11:19:39,009 : - storing 12924x300 projection weights into ./../models/model_skipgram.txt\n"
     ]
    }
   ],
   "source": [
    "model.wv.save_word2vec_format(\"./../models/model_cbow.txt\", binary=False)\n",
    "model_sg.wv.save_word2vec_format(\"./../models/model_skipgram.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 11:19:43,773 : - loading projection weights from ./../models/model_cbow.txt\n",
      "2024-12-03 11:19:44,809 : - KeyedVectors lifecycle event {'msg': 'loaded (12924, 300) matrix of type float32 from ./../models/model_cbow.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-12-03T11:19:44.809474', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n",
      "2024-12-03 11:19:44,811 : - loading projection weights from ./../models/model_skipgram.txt\n",
      "2024-12-03 11:19:45,843 : - KeyedVectors lifecycle event {'msg': 'loaded (12924, 300) matrix of type float32 from ./../models/model_skipgram.txt', 'binary': False, 'encoding': 'utf8', 'datetime': '2024-12-03T11:19:45.843738', 'gensim': '4.3.3', 'python': '3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]', 'platform': 'macOS-14.6.1-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "cbow_model = KeyedVectors.load_word2vec_format(\"./../models/model_cbow.txt\")\n",
    "skipgram_model = KeyedVectors.load_word2vec_format(\"./../models/model_skipgram.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\", disable=[\"paser\", \"ner\", \"tagger\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 300)\n",
      "(20513, 300)\n"
     ]
    }
   ],
   "source": [
    "cbow_vectors_train = get_vectors(df_train[\"title\"], cbow_model, nlp)\n",
    "cbow_vectors_test = get_vectors(df_test[\"title\"], cbow_model, nlp)\n",
    "\n",
    "print(cbow_vectors_train.shape)\n",
    "print(cbow_vectors_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000, 300)\n",
      "(20513, 300)\n"
     ]
    }
   ],
   "source": [
    "skipgram_vectors_train = get_vectors(df_train[\"title\"], skipgram_model, nlp)\n",
    "skipgram_vectors_test = get_vectors(df_test[\"title\"], skipgram_model, nlp)\n",
    "\n",
    "print(skipgram_vectors_train.shape)\n",
    "print(skipgram_vectors_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.80      0.71      0.75      6103\n",
      "   cotidiano       0.64      0.80      0.71      1698\n",
      "     esporte       0.93      0.87      0.90      4663\n",
      "   ilustrada       0.13      0.86      0.23       131\n",
      "     mercado       0.84      0.78      0.81      5867\n",
      "       mundo       0.74      0.83      0.79      2051\n",
      "\n",
      "    accuracy                           0.79     20513\n",
      "   macro avg       0.68      0.81      0.70     20513\n",
      "weighted avg       0.82      0.79      0.80     20513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cbow_lr_model = lr_classifier(cbow_vectors_train, df_train[\"category\"], cbow_vectors_test, df_test[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     colunas       0.81      0.72      0.76      6103\n",
      "   cotidiano       0.64      0.81      0.71      1698\n",
      "     esporte       0.93      0.88      0.90      4663\n",
      "   ilustrada       0.14      0.87      0.24       131\n",
      "     mercado       0.84      0.79      0.81      5867\n",
      "       mundo       0.76      0.84      0.80      2051\n",
      "\n",
      "    accuracy                           0.79     20513\n",
      "   macro avg       0.69      0.82      0.70     20513\n",
      "weighted avg       0.82      0.79      0.80     20513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skipgram_lr_model = lr_classifier(skipgram_vectors_train, df_train[\"category\"], skipgram_vectors_test, df_test[\"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./../models/skipgram_lr_model.joblib']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(cbow_lr_model, \"./../models/cbow_lr_model.joblib\")\n",
    "dump(skipgram_lr_model, \"./../models/skipgram_lr_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
